diff --git a/cmake/OpenCVCompilerOptimizations.cmake b/cmake/OpenCVCompilerOptimizations.cmake
index 0c77f4505..cc57cbdca 100644
--- a/cmake/OpenCVCompilerOptimizations.cmake
+++ b/cmake/OpenCVCompilerOptimizations.cmake
@@ -26,7 +26,7 @@
 #
 # CPU_DISPATCH_FLAGS_${opt} - flags for source files compiled separately (<name>.avx2.cpp)
 
-set(CPU_ALL_OPTIMIZATIONS "SSE;SSE2;SSE3;SSSE3;SSE4_1;SSE4_2;POPCNT;AVX;FP16;AVX2;FMA3") # without AVX512
+set(CPU_ALL_OPTIMIZATIONS "SSE;SSE2;SSE3;SSSE3;SSE4_1;SSE4_2;POPCNT;AVX;FP16;AVX2;FMA3;AVX512")
 list(APPEND CPU_ALL_OPTIMIZATIONS NEON VFPV3 FP16)
 list(APPEND CPU_ALL_OPTIMIZATIONS VSX)
 list(REMOVE_DUPLICATES CPU_ALL_OPTIMIZATIONS)
@@ -145,7 +145,7 @@ elseif(" ${CMAKE_CXX_FLAGS} " MATCHES " -march=native | -xHost | /QxHost ")
 endif()
 
 if(X86 OR X86_64)
-  ocv_update(CPU_KNOWN_OPTIMIZATIONS "SSE;SSE2;SSE3;SSSE3;SSE4_1;POPCNT;SSE4_2;FP16;FMA3;AVX;AVX2") # without AVX512
+  ocv_update(CPU_KNOWN_OPTIMIZATIONS "SSE;SSE2;SSE3;SSSE3;SSE4_1;POPCNT;SSE4_2;FP16;FMA3;AVX;AVX2;AVX512")
 
   ocv_update(CPU_SSE_TEST_FILE "${OpenCV_SOURCE_DIR}/cmake/checks/cpu_sse.cpp")
   ocv_update(CPU_SSE2_TEST_FILE "${OpenCV_SOURCE_DIR}/cmake/checks/cpu_sse2.cpp")
diff --git a/modules/core/include/opencv2/core/cv_cpu_dispatch.h b/modules/core/include/opencv2/core/cv_cpu_dispatch.h
index 5261a4143..75f6ca9c7 100644
--- a/modules/core/include/opencv2/core/cv_cpu_dispatch.h
+++ b/modules/core/include/opencv2/core/cv_cpu_dispatch.h
@@ -82,6 +82,10 @@
 #  include <immintrin.h>
 #  define CV_AVX2 1
 #endif
+#ifdef CV_CPU_COMPILE_AVX512
+#  include <immintrin.h>
+#  define CV_AVX512 1
+#endif
 #ifdef CV_CPU_COMPILE_FMA3
 #  define CV_FMA3 1
 #endif
diff --git a/modules/core/include/opencv2/core/cv_cpu_helper.h b/modules/core/include/opencv2/core/cv_cpu_helper.h
index 66a473f1e..1c7dbaf85 100644
--- a/modules/core/include/opencv2/core/cv_cpu_helper.h
+++ b/modules/core/include/opencv2/core/cv_cpu_helper.h
@@ -165,6 +165,21 @@
 #endif
 #define __CV_CPU_DISPATCH_CHAIN_FMA3(fn, args, mode, ...)  CV_CPU_CALL_FMA3(fn, args); __CV_EXPAND(__CV_CPU_DISPATCH_CHAIN_ ## mode(fn, args, __VA_ARGS__))
 
+#if !defined CV_DISABLE_OPTIMIZATION && defined CV_ENABLE_INTRINSICS && defined CV_CPU_COMPILE_AVX512
+#  define CV_TRY_AVX512 1
+#  define CV_CPU_HAS_SUPPORT_AVX512 1
+#  define CV_CPU_CALL_AVX512(fn, args) return (opt_AVX512::fn args)
+#elif !defined CV_DISABLE_OPTIMIZATION && defined CV_ENABLE_INTRINSICS && defined CV_CPU_DISPATCH_COMPILE_AVX512
+#  define CV_TRY_AVX512 1
+#  define CV_CPU_HAS_SUPPORT_AVX512 (cv::checkHardwareSupport(CV_CPU_AVX512))
+#  define CV_CPU_CALL_AVX512(fn, args) if (CV_CPU_HAS_SUPPORT_AVX512) return (opt_AVX512::fn args)
+#else
+#  define CV_TRY_AVX512 0
+#  define CV_CPU_HAS_SUPPORT_AVX512 0
+#  define CV_CPU_CALL_AVX512(fn, args)
+#endif
+#define __CV_CPU_DISPATCH_CHAIN_AVX512(fn, args, mode, ...)  CV_CPU_CALL_AVX512(fn, args); __CV_EXPAND(__CV_CPU_DISPATCH_CHAIN_ ## mode(fn, args, __VA_ARGS__))
+
 #if !defined CV_DISABLE_OPTIMIZATION && defined CV_ENABLE_INTRINSICS && defined CV_CPU_COMPILE_NEON
 #  define CV_TRY_NEON 1
 #  define CV_CPU_HAS_SUPPORT_NEON 1
diff --git a/modules/dnn/CMakeLists.txt b/modules/dnn/CMakeLists.txt
index 77c62475a..74723beb4 100644
--- a/modules/dnn/CMakeLists.txt
+++ b/modules/dnn/CMakeLists.txt
@@ -13,7 +13,7 @@ endif()
 
 set(the_description "Deep neural network module. It allows to load models from different frameworks and to make forward pass")
 
-ocv_add_dispatched_file("layers/layers_common" AVX AVX2)
+ocv_add_dispatched_file("layers/layers_common" AVX AVX2 AVX512)
 
 ocv_add_module(dnn opencv_core opencv_imgproc WRAP python matlab java js)
 ocv_warnings_disable(CMAKE_CXX_FLAGS -Wno-shadow -Wno-parentheses -Wmaybe-uninitialized -Wsign-promo
diff --git a/modules/dnn/src/layers/convolution_layer.cpp b/modules/dnn/src/layers/convolution_layer.cpp
index f650afab7..e1e6fa440 100644
--- a/modules/dnn/src/layers/convolution_layer.cpp
+++ b/modules/dnn/src/layers/convolution_layer.cpp
@@ -345,10 +345,11 @@ public:
         bool is1x1_;
         bool useAVX;
         bool useAVX2;
+        bool useAVX512;
 
         ParallelConv()
             : input_(0), weights_(0), output_(0), ngroups_(0), nstripes_(0),
-              biasvec_(0), reluslope_(0), activ_(0), is1x1_(false), useAVX(false), useAVX2(false)
+              biasvec_(0), reluslope_(0), activ_(0), is1x1_(false), useAVX(false), useAVX2(false), useAVX512(false)
         {}
 
         static void run( const Mat& input, Mat& output, const Mat& weights,
@@ -383,6 +384,7 @@ public:
             p.is1x1_ = kernel == Size(0,0) && pad == Size(0, 0);
             p.useAVX = checkHardwareSupport(CPU_AVX);
             p.useAVX2 = checkHardwareSupport(CPU_AVX2);
+            p.useAVX512 = checkHardwareSupport(CPU_AVX_512DQ);
 
             int ncn = std::min(inpCn, (int)BLK_SIZE_CN);
             p.ofstab_.resize(kernel.width*kernel.height*ncn);
@@ -562,6 +564,13 @@ public:
                         // now compute dot product of the weights
                         // and im2row-transformed part of the tensor
                         int bsz = ofs1 - ofs0;
+                    #if CV_TRY_AVX512
+                        /* AVX512 convolution requires an alignment of 16, and ROI is only there for larger vector sizes */
+                        if(useAVX512 && (vsz_a & 15) == 0 && vsz >= 64)
+                            opt_AVX512::fastConv(wptr, wstep, biasptr, rowbuf0, data_out0 + ofs0,
+                                          outShape, bsz, vsz, vsz_a, relu, cn0 == 0);
+                        else
+                    #endif
                     #if CV_TRY_AVX2
                         if(useAVX2)
                             opt_AVX2::fastConv(wptr, wstep, biasptr, rowbuf0, data_out0 + ofs0,
@@ -1093,6 +1102,7 @@ public:
             nstripes_ = nstripes;
             useAVX = checkHardwareSupport(CPU_AVX);
             useAVX2 = checkHardwareSupport(CPU_AVX2);
+            useAVX512 = checkHardwareSupport(CPU_AVX_512DQ);
         }
 
         void operator()(const Range& range_) const
@@ -1110,6 +1120,11 @@ public:
             size_t bstep = b_->step1();
             size_t cstep = c_->step1();
 
+        #if CV_TRY_AVX512
+            if( useAVX512 )
+                opt_AVX512::fastGEMM( aptr, astep, bptr, bstep, cptr, cstep, mmax, kmax, nmax );
+            else
+        #endif
         #if CV_TRY_AVX2
             if( useAVX2 )
                 opt_AVX2::fastGEMM( aptr, astep, bptr, bstep, cptr, cstep, mmax, kmax, nmax );
@@ -1214,6 +1229,7 @@ public:
         int nstripes_;
         bool useAVX;
         bool useAVX2;
+        bool useAVX512;
     };
 
     class Col2ImInvoker : public cv::ParallelLoopBody
diff --git a/modules/dnn/src/layers/layers_common.simd.hpp b/modules/dnn/src/layers/layers_common.simd.hpp
index 72a0f343f..7c089483b 100644
--- a/modules/dnn/src/layers/layers_common.simd.hpp
+++ b/modules/dnn/src/layers/layers_common.simd.hpp
@@ -57,7 +57,7 @@ void fastGEMM( const float* aptr, size_t astep, const float* bptr,
                size_t bstep, float* cptr, size_t cstep,
                int ma, int na, int nb );
 
-#if !defined(CV_CPU_OPTIMIZATION_DECLARATIONS_ONLY) && CV_AVX
+#if !defined(CV_CPU_OPTIMIZATION_DECLARATIONS_ONLY) && CV_AVX && !CV_AVX512
 
 #if !CV_FMA3 // AVX workaround
 #undef _mm256_fmadd_ps
@@ -366,5 +366,404 @@ void fastGEMM( const float* aptr, size_t astep, const float* bptr,
 
 #endif // CV_CPU_OPTIMIZATION_DECLARATIONS_ONLY
 
+#if !defined(CV_CPU_OPTIMIZATION_DECLARATIONS_ONLY) && CV_AVX512
+
+void fastConv( const float* weights, size_t wstep, const float* bias,
+               const float* rowbuf, float* output, const int* outShape,
+               int blockSize, int vecsize, int vecsize_aligned,
+               const float* relu, bool initOutput )
+{
+    int outCn = outShape[1];
+    size_t outPlaneSize = outShape[2]*outShape[3];
+    float r0 = 1.f, r1 = 1.f, r2 = 1.f;
+    __m128 vr0 = _mm_set1_ps(1.f), vr1 = vr0, vr2 = vr0;
+    __m128 z = _mm_setzero_ps();
+
+    // now compute dot product of the weights
+    // and im2row-transformed part of the tensor
+    for( int i = 0; i < outCn; i += 3 )
+    {
+        const float* wptr0 = weights + i*wstep;
+        const float* wptr1 = wptr0 + wstep;
+        const float* wptr2 = wptr1 + wstep;
+        float* outptr0 = output + i*outPlaneSize;
+        float* outptr1 = outptr0 + outPlaneSize;
+        float* outptr2 = outptr1 + outPlaneSize;
+        float bias0 = bias[i], bias1 = bias[i+1], bias2 = bias[i+2];
+
+        if( i+2 >= outCn )
+        {
+            wptr2 = wptr1;
+            outptr2 = outptr1;
+            bias2 = bias1;
+            if( i+1 >= outCn )
+            {
+                wptr2 = wptr1 = wptr0;
+                outptr2 = outptr1 = outptr0;
+                bias2 = bias1 = bias0;
+            }
+        }
+
+        if( relu )
+        {
+            r0 = relu[i];
+            r1 = relu[i+1];
+            r2 = relu[i+2];
+            vr0 = _mm_set1_ps(r0);
+            vr1 = _mm_set1_ps(r1);
+            vr2 = _mm_set1_ps(r2);
+        }
+
+        int j = 0;
+        for( ; j <= blockSize - 4; j += 4 )
+        {
+            const float* rptr = rowbuf + j*vecsize_aligned;
+            float tempa[4];
+
+            __m512 vs00 = _mm512_setzero_ps(), vs01 = _mm512_setzero_ps(),
+                   vs02 = _mm512_setzero_ps(), vs03 = _mm512_setzero_ps(),
+                   vs10 = _mm512_setzero_ps(), vs11 = _mm512_setzero_ps(),
+                   vs12 = _mm512_setzero_ps(), vs13 = _mm512_setzero_ps(),
+                   vs20 = _mm512_setzero_ps(), vs21 = _mm512_setzero_ps(),
+                   vs22 = _mm512_setzero_ps(), vs23 = _mm512_setzero_ps();
+
+            for( int k = 0; k < vecsize_aligned; k += 16, rptr += 16 )
+            {
+                __m512 w0 = _mm512_load_ps(wptr0 + k);
+                __m512 w1 = _mm512_load_ps(wptr1 + k);
+                __m512 w2 = _mm512_load_ps(wptr2 + k);
+                __m512 r0 = _mm512_load_ps(rptr);
+
+                vs00 = _mm512_fmadd_ps(w0, r0, vs00);
+                vs10 = _mm512_fmadd_ps(w1, r0, vs10);
+                vs20 = _mm512_fmadd_ps(w2, r0, vs20);
+
+                r0 = _mm512_load_ps(rptr + vecsize_aligned);
+                vs01 = _mm512_fmadd_ps(w0, r0, vs01);
+                vs11 = _mm512_fmadd_ps(w1, r0, vs11);
+                vs21 = _mm512_fmadd_ps(w2, r0, vs21);
+
+                r0 = _mm512_load_ps(rptr + vecsize_aligned*2);
+                vs02 = _mm512_fmadd_ps(w0, r0, vs02);
+                vs12 = _mm512_fmadd_ps(w1, r0, vs12);
+                vs22 = _mm512_fmadd_ps(w2, r0, vs22);
+
+                r0 = _mm512_load_ps(rptr + vecsize_aligned*3);
+                vs03 = _mm512_fmadd_ps(w0, r0, vs03);
+                vs13 = _mm512_fmadd_ps(w1, r0, vs13);
+                vs23 = _mm512_fmadd_ps(w2, r0, vs23);
+            }
+
+
+            tempa[0] = _mm512_reduce_add_ps(vs00);
+            tempa[1] = _mm512_reduce_add_ps(vs01);
+            tempa[2] = _mm512_reduce_add_ps(vs02);
+            tempa[3] = _mm512_reduce_add_ps(vs03);
+            __m128 t0 = _mm_load_ps(&tempa[0]);
+            tempa[0] = _mm512_reduce_add_ps(vs10);
+            tempa[1] = _mm512_reduce_add_ps(vs11);
+            tempa[2] = _mm512_reduce_add_ps(vs12);
+            tempa[3] = _mm512_reduce_add_ps(vs13);
+            __m128 t1 = _mm_load_ps(&tempa[0]);
+            tempa[0] = _mm512_reduce_add_ps(vs20);
+            tempa[1] = _mm512_reduce_add_ps(vs21);
+            tempa[2] = _mm512_reduce_add_ps(vs22);
+            tempa[3] = _mm512_reduce_add_ps(vs23);
+            __m128 t2 = _mm_load_ps(&tempa[0]);
+
+            __m128 s0, s1, s2;
+
+            if( initOutput )
+            {
+                s0 = _mm_set1_ps(bias0);
+                s1 = _mm_set1_ps(bias1);
+                s2 = _mm_set1_ps(bias2);
+            }
+            else
+            {
+                s0 = _mm_loadu_ps(outptr0 + j);
+                s1 = _mm_loadu_ps(outptr1 + j);
+                s2 = _mm_loadu_ps(outptr2 + j);
+            }
+
+            s0 = _mm_add_ps(s0, t0);
+            s1 = _mm_add_ps(s1, t1);
+            s2 = _mm_add_ps(s2, t2);
+
+            if( relu )
+            {
+                __m128 m0 = _mm_cmp_ps(s0, z, _CMP_GT_OS);
+                __m128 m1 = _mm_cmp_ps(s1, z, _CMP_GT_OS);
+                __m128 m2 = _mm_cmp_ps(s2, z, _CMP_GT_OS);
+                s0 = _mm_xor_ps(s0, _mm_andnot_ps(m0, _mm_xor_ps(_mm_mul_ps(s0, vr0), s0)));
+                s1 = _mm_xor_ps(s1, _mm_andnot_ps(m1, _mm_xor_ps(_mm_mul_ps(s1, vr1), s1)));
+                s2 = _mm_xor_ps(s2, _mm_andnot_ps(m2, _mm_xor_ps(_mm_mul_ps(s2, vr2), s2)));
+            }
+
+            _mm_storeu_ps(outptr0 + j, s0);
+            _mm_storeu_ps(outptr1 + j, s1);
+            _mm_storeu_ps(outptr2 + j, s2);
+        }
+
+        for( ; j < blockSize; j++ )
+        {
+            const float* rptr = rowbuf + j*vecsize_aligned;
+            float s00, s10, s20;
+
+            if( initOutput )
+            {
+                s00 = bias0;
+                s10 = bias1;
+                s20 = bias2;
+            }
+            else
+            {
+                s00 = outptr0[j];
+                s10 = outptr1[j];
+                s20 = outptr2[j];
+            }
+
+            for( int k = 0; k < vecsize; k++ )
+            {
+                float r0 = rptr[k];
+                s00 += wptr0[k]*r0;
+                s10 += wptr1[k]*r0;
+                s20 += wptr2[k]*r0;
+            }
+
+            if( relu )
+            {
+                s00 = s00 > 0.f ? s00 : s00*r0;
+                s10 = s10 > 0.f ? s10 : s10*r1;
+                s20 = s20 > 0.f ? s20 : s20*r2;
+            }
+
+            outptr0[j] = s00;
+            outptr1[j] = s10;
+            outptr2[j] = s20;
+        }
+    }
+    _mm256_zeroupper();
+}
+
+// dst = vec * weights^t + bias
+void fastGEMM1T( const float* vec, const float* weights,
+                 size_t wstep, const float* bias,
+                 float* dst, int nvecs, int vecsize )
+{
+    int i = 0;
+
+    for( ; i <= nvecs - 16; i += 16 )
+    {
+        const float* wptr = weights + i*wstep;
+        __m256 vs0 = _mm256_setzero_ps(), vs1 = _mm256_setzero_ps(),
+               vs2 = _mm256_setzero_ps(), vs3 = _mm256_setzero_ps(),
+               vs4 = _mm256_setzero_ps(), vs5 = _mm256_setzero_ps(),
+               vs6 = _mm256_setzero_ps(), vs7 = _mm256_setzero_ps();
+
+        for( int k = 0; k < vecsize; k += 8, wptr += 8 )
+        {
+            __m256 v = _mm256_load_ps(vec + k);
+
+            vs0 = _mm256_fmadd_ps(_mm256_load_ps(wptr), v, vs0);
+            vs1 = _mm256_fmadd_ps(_mm256_load_ps(wptr + wstep), v, vs1);
+            vs2 = _mm256_fmadd_ps(_mm256_load_ps(wptr + wstep*2), v, vs2);
+            vs3 = _mm256_fmadd_ps(_mm256_load_ps(wptr + wstep*3), v, vs3);
+            vs4 = _mm256_fmadd_ps(_mm256_load_ps(wptr + wstep*4), v, vs4);
+            vs5 = _mm256_fmadd_ps(_mm256_load_ps(wptr + wstep*5), v, vs5);
+            vs6 = _mm256_fmadd_ps(_mm256_load_ps(wptr + wstep*6), v, vs6);
+            vs7 = _mm256_fmadd_ps(_mm256_load_ps(wptr + wstep*7), v, vs7);
+        }
+
+        __m256 s0 = _mm256_hadd_ps(_mm256_hadd_ps(vs0, vs1), _mm256_hadd_ps(vs2, vs3));
+        __m256 s1 = _mm256_hadd_ps(_mm256_hadd_ps(vs4, vs5), _mm256_hadd_ps(vs6, vs7));
+
+        s0 = _mm256_add_ps(s0, _mm256_permute2f128_ps(s0, s0, 1));
+        s1 = _mm256_add_ps(s1, _mm256_permute2f128_ps(s1, s1, 1));
+
+        s0 = _mm256_add_ps(s0, _mm256_castps128_ps256(_mm_loadu_ps(bias + i)));
+        s1 = _mm256_add_ps(s1, _mm256_castps128_ps256(_mm_loadu_ps(bias + i + 8)));
+
+        _mm_storeu_ps(dst + i, _mm256_castps256_ps128(s0));
+        _mm_storeu_ps(dst + i + 4, _mm256_castps256_ps128(s1));
+    }
+
+    for( ; i <= nvecs - 8; i += 8 )
+    {
+        const float* wptr = weights + i*wstep;
+        __m256 vs0 = _mm256_setzero_ps(), vs1 = _mm256_setzero_ps(),
+               vs2 = _mm256_setzero_ps(), vs3 = _mm256_setzero_ps(),
+               vs4 = _mm256_setzero_ps(), vs5 = _mm256_setzero_ps(),
+               vs6 = _mm256_setzero_ps(), vs7 = _mm256_setzero_ps();
+
+        for( int k = 0; k < vecsize; k += 8, wptr += 8 )
+        {
+            __m256 v = _mm256_load_ps(vec + k);
+
+            vs0 = _mm256_fmadd_ps(_mm256_load_ps(wptr), v, vs0);
+            vs1 = _mm256_fmadd_ps(_mm256_load_ps(wptr + wstep), v, vs1);
+            vs2 = _mm256_fmadd_ps(_mm256_load_ps(wptr + wstep*2), v, vs2);
+            vs3 = _mm256_fmadd_ps(_mm256_load_ps(wptr + wstep*3), v, vs3);
+            vs4 = _mm256_fmadd_ps(_mm256_load_ps(wptr + wstep*4), v, vs4);
+            vs5 = _mm256_fmadd_ps(_mm256_load_ps(wptr + wstep*5), v, vs5);
+            vs6 = _mm256_fmadd_ps(_mm256_load_ps(wptr + wstep*6), v, vs6);
+            vs7 = _mm256_fmadd_ps(_mm256_load_ps(wptr + wstep*7), v, vs7);
+        }
+
+        __m256 s0 = _mm256_hadd_ps(_mm256_hadd_ps(vs0, vs1), _mm256_hadd_ps(vs2, vs3));
+        __m256 s1 = _mm256_hadd_ps(_mm256_hadd_ps(vs4, vs5), _mm256_hadd_ps(vs6, vs7));
+
+        s0 = _mm256_add_ps(s0, _mm256_permute2f128_ps(s0, s0, 1));
+        s1 = _mm256_add_ps(s1, _mm256_permute2f128_ps(s1, s1, 1));
+
+        s0 = _mm256_add_ps(s0, _mm256_castps128_ps256(_mm_loadu_ps(bias + i)));
+        s1 = _mm256_add_ps(s1, _mm256_castps128_ps256(_mm_loadu_ps(bias + i + 4)));
+
+        _mm_storeu_ps(dst + i, _mm256_castps256_ps128(s0));
+        _mm_storeu_ps(dst + i + 4, _mm256_castps256_ps128(s1));
+    }
+
+    float temp = 0.f;
+    for( ; i < nvecs; i++ )
+    {
+        const float* wptr = weights + i*wstep;
+        __m256 vs0 = _mm256_setzero_ps();
+
+        for( int k = 0; k < vecsize; k += 8, wptr += 8 )
+        {
+            __m256 v = _mm256_load_ps(vec + k);
+            vs0 = _mm256_fmadd_ps(_mm256_load_ps(wptr), v, vs0);
+        }
+
+        __m256 s0 = _mm256_hadd_ps(_mm256_hadd_ps(vs0, vs0), vs0);
+        s0 = _mm256_add_ps(s0, _mm256_permute2f128_ps(s0, s0, 1));
+        _mm_store_ss(&temp, _mm256_castps256_ps128(s0));
+        dst[i] = temp + bias[i];
+    }
+
+    _mm256_zeroupper();
+}
+
+void fastGEMM( const float* aptr, size_t astep, const float* bptr,
+               size_t bstep, float* cptr, size_t cstep,
+               int ma, int na, int nb )
+{
+    int n = 0;
+    for( ; n <= nb - 32; n += 32 )
+    {
+        for( int m = 0; m < ma; m += 4 )
+        {
+            const float* aptr0 = aptr + astep*m;
+            const float* aptr1 = aptr + astep*std::min(m+1, ma-1);
+            const float* aptr2 = aptr + astep*std::min(m+2, ma-1);
+            const float* aptr3 = aptr + astep*std::min(m+3, ma-1);
+
+            float* cptr0 = cptr + cstep*m;
+            float* cptr1 = cptr + cstep*std::min(m+1, ma-1);
+            float* cptr2 = cptr + cstep*std::min(m+2, ma-1);
+            float* cptr3 = cptr + cstep*std::min(m+3, ma-1);
+
+            __m512 d00 = _mm512_setzero_ps(), d01 = _mm512_setzero_ps();
+            __m512 d10 = _mm512_setzero_ps(), d11 = _mm512_setzero_ps();
+            __m512 d20 = _mm512_setzero_ps(), d21 = _mm512_setzero_ps();
+            __m512 d30 = _mm512_setzero_ps(), d31 = _mm512_setzero_ps();
+
+            for( int k = 0; k < na; k++ )
+            {
+                __m512 a0 = _mm512_set1_ps(aptr0[k]);
+                __m512 a1 = _mm512_set1_ps(aptr1[k]);
+                __m512 a2 = _mm512_set1_ps(aptr2[k]);
+                __m512 a3 = _mm512_set1_ps(aptr3[k]);
+                __m512 b0 = _mm512_loadu_ps(bptr + k*bstep + n);
+                __m512 b1 = _mm512_loadu_ps(bptr + k*bstep + n + 16);
+                d00 = _mm512_fmadd_ps(a0, b0, d00);
+                d01 = _mm512_fmadd_ps(a0, b1, d01);
+                d10 = _mm512_fmadd_ps(a1, b0, d10);
+                d11 = _mm512_fmadd_ps(a1, b1, d11);
+                d20 = _mm512_fmadd_ps(a2, b0, d20);
+                d21 = _mm512_fmadd_ps(a2, b1, d21);
+                d30 = _mm512_fmadd_ps(a3, b0, d30);
+                d31 = _mm512_fmadd_ps(a3, b1, d31);
+            }
+
+            _mm512_storeu_ps(cptr0 + n, d00);
+            _mm512_storeu_ps(cptr0 + n + 16, d01);
+            _mm512_storeu_ps(cptr1 + n, d10);
+            _mm512_storeu_ps(cptr1 + n + 16, d11);
+            _mm512_storeu_ps(cptr2 + n, d20);
+            _mm512_storeu_ps(cptr2 + n + 16, d21);
+            _mm512_storeu_ps(cptr3 + n, d30);
+            _mm512_storeu_ps(cptr3 + n + 16, d31);
+        }
+    }
+
+
+    for( ; n <= nb - 16; n += 16 )
+    {
+        for( int m = 0; m < ma; m += 4 )
+        {
+            const float* aptr0 = aptr + astep*m;
+            const float* aptr1 = aptr + astep*std::min(m+1, ma-1);
+            const float* aptr2 = aptr + astep*std::min(m+2, ma-1);
+            const float* aptr3 = aptr + astep*std::min(m+3, ma-1);
+
+            float* cptr0 = cptr + cstep*m;
+            float* cptr1 = cptr + cstep*std::min(m+1, ma-1);
+            float* cptr2 = cptr + cstep*std::min(m+2, ma-1);
+            float* cptr3 = cptr + cstep*std::min(m+3, ma-1);
+
+            __m256 d00 = _mm256_setzero_ps(), d01 = _mm256_setzero_ps();
+            __m256 d10 = _mm256_setzero_ps(), d11 = _mm256_setzero_ps();
+            __m256 d20 = _mm256_setzero_ps(), d21 = _mm256_setzero_ps();
+            __m256 d30 = _mm256_setzero_ps(), d31 = _mm256_setzero_ps();
+
+            for( int k = 0; k < na; k++ )
+            {
+                __m256 a0 = _mm256_set1_ps(aptr0[k]);
+                __m256 a1 = _mm256_set1_ps(aptr1[k]);
+                __m256 a2 = _mm256_set1_ps(aptr2[k]);
+                __m256 a3 = _mm256_set1_ps(aptr3[k]);
+                __m256 b0 = _mm256_loadu_ps(bptr + k*bstep + n);
+                __m256 b1 = _mm256_loadu_ps(bptr + k*bstep + n + 8);
+                d00 = _mm256_fmadd_ps(a0, b0, d00);
+                d01 = _mm256_fmadd_ps(a0, b1, d01);
+                d10 = _mm256_fmadd_ps(a1, b0, d10);
+                d11 = _mm256_fmadd_ps(a1, b1, d11);
+                d20 = _mm256_fmadd_ps(a2, b0, d20);
+                d21 = _mm256_fmadd_ps(a2, b1, d21);
+                d30 = _mm256_fmadd_ps(a3, b0, d30);
+                d31 = _mm256_fmadd_ps(a3, b1, d31);
+            }
+
+            _mm256_storeu_ps(cptr0 + n, d00);
+            _mm256_storeu_ps(cptr0 + n + 8, d01);
+            _mm256_storeu_ps(cptr1 + n, d10);
+            _mm256_storeu_ps(cptr1 + n + 8, d11);
+            _mm256_storeu_ps(cptr2 + n, d20);
+            _mm256_storeu_ps(cptr2 + n + 8, d21);
+            _mm256_storeu_ps(cptr3 + n, d30);
+            _mm256_storeu_ps(cptr3 + n + 8, d31);
+        }
+    }
+
+    for( ; n < nb; n++ )
+    {
+        for( int m = 0; m < ma; m++ )
+        {
+            const float* aptr0 = aptr + astep*m;
+            float* cptr0 = cptr + cstep*m;
+            float d0 = 0.f;
+
+            for( int k = 0; k < na; k++ )
+                d0 += aptr0[k]*bptr[k*bstep + n];
+
+            cptr0[n] = d0;
+        }
+    }
+    _mm256_zeroupper();
+}
+
+#endif // CV_CPU_OPTIMIZATION_DECLARATIONS_ONLY
+
 CV_CPU_OPTIMIZATION_NAMESPACE_END
 }} // namespace
